{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pandas import DataFrame, read_csv\n",
    "# read_csv()\n",
    "import os\n",
    "\n",
    "BLOCK_SIZE = 20\n",
    "FILTER_THRESHOLD = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, defaultdict\n",
    "\n",
    "\n",
    "# Point = namedtuple('Point',['window', 'snp', 'bp'])\n",
    "\n",
    "class Point:\n",
    "    def __init__(self, snp, bp=0):\n",
    "        self.snp, self.bp = snp, bp\n",
    "    \n",
    "    @property\n",
    "    def window(self):\n",
    "        return self.snp // BLOCK_SIZE\n",
    "\n",
    "# class Specimen:\n",
    "#     def __init__(self, ident, sequence)\n",
    "#         ident, sequence\n",
    "    \n",
    "class Node:\n",
    "    def __init__(self, ident, start, end, upstream=None, downstream=None, specimens=None):\n",
    "        self.ident = ident\n",
    "        self.start = start #Point()\n",
    "        self.end = end #Point()\n",
    "        # {nothing_node:501, Node: 38,  Node: 201, Node: 3}\n",
    "        self.upstream = defaultdict(lambda: 0) if not upstream else upstream \n",
    "        # {Node: 38,  Node: 201, Node: 3}\n",
    "        self.downstream = defaultdict(lambda: 0) if not downstream else downstream\n",
    "        self.specimens = set() if specimens is None else specimens\n",
    "        assert self.start is not None and self.end is not None, self.details()\n",
    "        assert self.end.snp is not None or (self.end.snp is None and self.start.snp is None), self.details()\n",
    "#        assert sum(self.upstream.values()) is len(self.specimens), self.details()\n",
    "#        assert sum(self.downstream.values()) is len(self.specimens), self.details()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.specimens)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"N%s(%s, %s)\" % (str(self.ident), str(self.start.snp), str(self.end.snp))\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.ident) + hash(self.start.snp) + hash(self.end.snp)\n",
    "    \n",
    "    def details(self):\n",
    "        return f\"\"\"Node{self.ident}: {self.start.snp} - {self.end.snp}\n",
    "        upstream: { dict((key, value) for key,value in self.upstream.items()) }\n",
    "        downstream: { dict((key, value) for key,value in self.downstream.items()) }\n",
    "        specimens: {self.specimens}\"\"\"\n",
    "        \n",
    "\n",
    "a = Point(0)\n",
    "b = Point(14)\n",
    "str(Node(57, a, b))\n",
    "nothing_node = Node(-1, Point(None), Point(None))\n",
    "global_nodes = {0: nothing_node}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deepcopy(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path = \"../test_data/KE_chromo10.txt\"):\n",
    "    \"\"\"Individuals are rows, not columns\"\"\"\n",
    "    loci = []\n",
    "    with open(file_path) as ke:\n",
    "        for line in ke.readlines():\n",
    "            loci.append(tuple(int(x) for x in line.split()))\n",
    "            \n",
    "    \n",
    "    individuals = np.array(loci).T.tolist()\n",
    "    return loci, individuals\n",
    "alleles, individuals = read_data()\n",
    "assert len(alleles) == 32767\n",
    "assert len(individuals[1]) == 32767\n",
    "assert len(individuals) == 501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first(iterable):\n",
    "    return next(iter(iterable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signature(individual, start_locus):\n",
    "    return tuple(individual[start_locus : start_locus + BLOCK_SIZE])\n",
    "\n",
    "def get_unique_signatures(individuals, start_locus, block_size = 20):\n",
    "    unique_blocks = {}\n",
    "    for individual in individuals:\n",
    "        sig = signature(individual, start_locus)\n",
    "        if sig not in unique_blocks:\n",
    "            unique_blocks[sig] = Node(len(unique_blocks), Point(start_locus // block_size, start_locus), \n",
    "                                      Point(start_locus // block_size, start_locus + BLOCK_SIZE)) #TODO: -1?\n",
    "    \n",
    "    return unique_blocks\n",
    "\n",
    "def test_get_unique_signatures(individuals):\n",
    "    unique_blocks = get_unique_signatures(individuals, 0 )\n",
    "    assert len(unique_blocks) == 4\n",
    "    assert unique_blocks.__repr__() == '{(0, 2, 0, 0, 2, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 2, 0, 0, 0, 0): N0(0, 0), '\\\n",
    "    '(0, 0, 2, 2, 0, 2, 0, 2, 2, 2, 0, 2, 2, 2, 0, 0, 2, 2, 2, 2): N1(0, 0), '\\\n",
    "    '(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0): N2(0, 0), '\\\n",
    "    '(2, 0, 2, 2, 0, 2, 0, 2, 2, 2, 0, 2, 2, 2, 0, 0, 2, 2, 2, 2): N3(0, 0)}'\n",
    "test_get_unique_signatures(individuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_signatures(alleles, individuals):\n",
    "    unique_signatures = []\n",
    "    for locus_start in range(0, len(alleles) - BLOCK_SIZE, BLOCK_SIZE):  # discards remainder \n",
    "        sig = get_unique_signatures(individuals, locus_start, BLOCK_SIZE)\n",
    "        unique_signatures.append(sig)\n",
    "    return unique_signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_individuals(individuals, unique_signatures):\n",
    "    simplified_individuals = []\n",
    "    for i_specimen, specimen in enumerate(individuals):\n",
    "        my_simplification = []\n",
    "        for w, window in enumerate(unique_signatures):  # the length of the genome\n",
    "            sig = signature(specimen, w * BLOCK_SIZE)\n",
    "    #         print(sig, unique_signatures[w][sig])\n",
    "    #         print(i_specimen, window)\n",
    "            my_simplification.append(unique_signatures[w][sig])\n",
    "        simplified_individuals.append(my_simplification)\n",
    "    return simplified_individuals\n",
    "\n",
    "def test_build_individuals(alleles, individuals):\n",
    "    unique_signatures = get_all_signatures(alleles, individuals)\n",
    "    assert repr(unique_signatures[21]) == '{(0, 0, 0, 0, 2, 2, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 2): N0(21, 21), (0, 0, 2, 0, 2, 2, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 2): N1(21, 21), (0, 0, 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0): N2(21, 21), (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0): N3(21, 21), (0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0): N4(21, 21), (0, 0, 0, 2, 2, 2, 0, 2, 0, 2, 2, 2, 2, 0, 0, 2, 0, 0, 0, 2): N5(21, 21), (0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0): N6(21, 21), (0, 0, 0, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 0, 2, 0, 0, 2, 2): N7(21, 21)}'\n",
    "    simplified_individuals = build_individuals(individuals, unique_signatures)\n",
    "    assert repr(simplified_individuals[500][:100]) == '[N2(0, 0), N2(1, 1), N2(2, 2), N2(3, 3), N2(4, 4), N2(5, 5), N3(6, 6), N3(7, 7), N3(8, 8), N2(9, 9), N0(10, 10), N1(11, 11), N2(12, 12), N2(13, 13), N2(14, 14), N2(15, 15), N3(16, 16), N3(17, 17), N4(18, 18), N3(19, 19), N5(20, 20), N3(21, 21), N3(22, 22), N10(23, 23), N4(24, 24), N3(25, 25), N4(26, 26), N3(27, 27), N1(28, 28), N1(29, 29), N4(30, 30), N3(31, 31), N21(32, 32), N1(33, 33), N1(34, 34), N1(35, 35), N1(36, 36), N1(37, 37), N1(38, 38), N1(39, 39), N1(40, 40), N1(41, 41), N1(42, 42), N1(43, 43), N1(44, 44), N1(45, 45), N1(46, 46), N1(47, 47), N1(48, 48), N1(49, 49), N1(50, 50), N1(51, 51), N1(52, 52), N1(53, 53), N1(54, 54), N1(55, 55), N1(56, 56), N1(57, 57), N1(58, 58), N1(59, 59), N1(60, 60), N1(61, 61), N1(62, 62), N1(63, 63), N1(64, 64), N1(65, 65), N1(66, 66), N1(67, 67), N1(68, 68), N1(69, 69), N1(70, 70), N1(71, 71), N1(72, 72), N1(73, 73), N1(74, 74), N1(75, 75), N1(76, 76), N1(77, 77), N0(78, 78), N0(79, 79), N1(80, 80), N1(81, 81), N1(82, 82), N1(83, 83), N1(84, 84), N1(85, 85), N1(86, 86), N1(87, 87), N1(88, 88), N1(89, 89), N1(90, 90), N1(91, 91), N1(92, 92), N1(93, 93), N1(94, 94), N1(95, 95), N1(96, 96), N1(97, 97), N0(98, 98), N0(99, 99)]'\n",
    "    assert len(simplified_individuals) == 501 and len(simplified_individuals[60]) == 1638\n",
    "test_build_individuals(alleles, individuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nodes: Populate upstream and downstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build nodes:  first 4 are the 4 starting signatures in window 0.  \n",
    "# For each node list which individuals are present at that node\n",
    "# List transition rates from one node to all other upstream and downstream\n",
    "def populate_transitions(simplified_individuals):\n",
    "    for i, indiv in enumerate(simplified_individuals):\n",
    "        # look what variants are present\n",
    "        for x, node in enumerate(indiv):\n",
    "            node.specimens.add(i)\n",
    "            if x + 1 < len(indiv):\n",
    "                node.downstream[indiv[x+1]] += 1\n",
    "            else:\n",
    "                node.downstream[nothing_node] += 1\n",
    "            if x-1 >= 0:\n",
    "                node.upstream[indiv[x-1]] += 1\n",
    "            else: \n",
    "                node.upstream[nothing_node] += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_signatures = get_all_signatures(alleles, individuals)\n",
    "simplified_individuals = build_individuals(individuals, unique_signatures)\n",
    "populate_transitions(simplified_individuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: turn these into tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "simplified_individuals[50][0].downstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplified_individuals[49][0].downstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplified_individuals[500][0].downstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplified_individuals[91][0].downstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x.downstream.values() for x in unique_signatures[1000].values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x.upstream.values() for x in unique_signatures[1000].values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add signature directly to node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from blist import blist\n",
    "blist = list\n",
    "from copy import copy, deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_no_duplicate_nodes(global_nodes):\n",
    "    unique_nodes = set()\n",
    "    for node in global_nodes:\n",
    "        if node in unique_nodes:\n",
    "            print(node)\n",
    "        else:\n",
    "            unique_nodes.add(node)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zoom_stack = [[]]\n",
    "def simple_merge(global_nodes):\n",
    "    new_layer = []  # TODO: copy old nodes to new layer conditionally\n",
    "    n = 0\n",
    "    while n < len(global_nodes):  # size of global_nodes changes, necessitating this weird loop\n",
    "        node = global_nodes[n]\n",
    "    #     print(node, type(node))\n",
    "        if len(node.downstream) == 1: \n",
    "            next_node = first(node.downstream.keys())\n",
    "            if len(node.specimens) == len(next_node.specimens):\n",
    "                #Torsten deletes nodeA and modifies next_node\n",
    "                next_node.upstream = node.upstream\n",
    "                next_node.start = node.start\n",
    "                #prepare to delete node by removing references\n",
    "                for parent in node.upstream.keys():\n",
    "                    if parent != nothing_node:\n",
    "                        count = parent.downstream[node]\n",
    "                        del parent.downstream[node]  # updating pointer \n",
    "                        parent.downstream[next_node] = count \n",
    "                global_nodes.remove(node)  #delete node\n",
    "                # zoom_stack[0].append(merged)\n",
    "                n -= 1\n",
    "        n += 1\n",
    "    return global_nodes        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_simple_merge(unique_signatures):\n",
    "    global_nodes = blist([node for window in unique_signatures for node in window.values()])  # think about referencing and deletion\n",
    "    assert len(global_nodes) == 7180\n",
    "    summary1 = simple_merge(global_nodes)\n",
    "    assert len(summary1) == 3690\n",
    "    return summary1\n",
    "summary1 = test_simple_merge(unique_signatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neglect Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_node(node, cutoff):\n",
    "    \"\"\"Changes references to this node to add to references to nothing_node\"\"\"\n",
    "    if cutoff < 1:\n",
    "        return  # if cutoff is 0, then don't touch upstream and downstream\n",
    "    for parent, count in node.upstream.items():\n",
    "        parent.downstream[nothing_node] += parent.downstream[node]\n",
    "        del parent.downstream[node]\n",
    "    for descendant, count in node.downstream.items():\n",
    "        descendant.upstream[nothing_node] += descendant.upstream[node]\n",
    "        del descendant.upstream[node]\n",
    "        \n",
    "\n",
    "def neglect_nodes(all_nodes, deletion_cutoff=FILTER_THRESHOLD):\n",
    "    nodes_to_delete = set()\n",
    "#     filtered_nodes = copy(all_nodes)\n",
    "#     filtered_nodes.remove(1)\n",
    "#     assert len(all_nodes) != len(filtered_nodes)\n",
    "    for node in all_nodes:\n",
    "        if len(node.specimens) <= deletion_cutoff:\n",
    "            delete_node(node, deletion_cutoff)  # TODO: check if this will orphan \n",
    "            nodes_to_delete.add(node)\n",
    "    filtered_nodes = blist([x for x in all_nodes if x not in nodes_to_delete])\n",
    "    # TODO: remove orphaned haplotypes in a node that transition to and from zero within a 10 window length\n",
    "    return filtered_nodes \n",
    "\n",
    "\n",
    "def test_neglect_nodes(all_nodes):\n",
    "    summary2 = neglect_nodes(all_nodes)\n",
    "    assert len(summary2) == 2854\n",
    "    unchanged = neglect_nodes(summary2, 0)\n",
    "    assert len([n for n in unchanged if len(n.specimens) == 0]) == 0\n",
    "    return summary2\n",
    "summary2 = test_neglect_nodes(summary1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary2[5].details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_one_group(prev_node, anchor, next_node):\n",
    "    \"\"\" Called when up.specimens == down.specimens\"\"\"\n",
    "    # Comment: That is actually the case we want to split up to obtain longer blocks later\n",
    "    # Extension of full windows will take care of potential loss of information later\n",
    "    my_specimens = copy(anchor.specimens)\n",
    "    if prev_node is not nothing_node:  # normal case\n",
    "        my_specimens = my_specimens.intersection(prev_node.specimens)\n",
    "    if next_node is not nothing_node:  # normal case\n",
    "        my_specimens = my_specimens.intersection(next_node.specimens)\n",
    "    if prev_node is nothing_node and next_node is nothing_node:  # exceptional: both are nothing node\n",
    "        my_specimens = copy(anchor.specimens)\n",
    "        for n in anchor.downstream.keys():\n",
    "            if n is not nothing_node:  # don't remove empty set\n",
    "                my_specimens -= n.specimens\n",
    "        for n in anchor.upstream.keys():\n",
    "            if n is not nothing_node:  # don't remove empty set\n",
    "                my_specimens -= n.specimens\n",
    "    \n",
    "    my_start, my_end = prev_node.start, next_node.end\n",
    "    my_upstream, my_downstream = prev_node.upstream, next_node.downstream\n",
    "    if nothing_node is prev_node:  # Rare case\n",
    "        my_start = anchor.start\n",
    "        my_upstream = anchor.upstream\n",
    "    if nothing_node is next_node:  # Rare case\n",
    "        my_end = anchor.end\n",
    "        my_downstream = anchor.downstream\n",
    "        \n",
    "    # TODO: what about case where more content is joining downstream?\n",
    "    new = Node(777, my_start, my_end, my_upstream, my_downstream, my_specimens)  \n",
    "    \n",
    "    \n",
    "    ## n.upstream/downstream contains the same key multiple times?!\n",
    "    ## My quick fix was to delete all upstream/downstream and just recalculate everything...\n",
    "    \n",
    "    \n",
    "    # Update upstream/downstream + specimens in prev_node, anchor, next_node\n",
    "    if prev_node != nothing_node:\n",
    "        prev_node.specimens -= new.specimens\n",
    "        prev_node = update_transition(prev_node)\n",
    "    \n",
    "    if next_node != nothing_node:\n",
    "        next_node.specimens -= new.specimens\n",
    "        next_node = update_transition(next_node)\n",
    "    \n",
    "    anchor.specimens -= new.specimens\n",
    "    anchor = update_transition(anchor)\n",
    "    \n",
    "    # Update upstream/downstream for new node\n",
    "    running = tuple(new.upstream.keys())\n",
    "    #new.upstream = defaultdict(lambda: 0) \n",
    "    for n in running:\n",
    "        if n != nothing_node:\n",
    "            new.upstream[n] = len(new.specimens.intersection(n.specimens))\n",
    "            n.downstream[new] = new.upstream[n]\n",
    "            update_transition(n)\n",
    "        update_transition(new)\n",
    "    \n",
    "    \n",
    "    running = tuple(new.downstream.keys())\n",
    "    #new.downstream = defaultdict(lambda: 0)\n",
    "    for n in running:\n",
    "        if n != nothing_node:\n",
    "            new.downstream[n] = len(new.specimens.intersection(n.specimens))\n",
    "            n.upstream[new] = new.downstream[n]\n",
    "            update_transition(n)\n",
    "        update_transition(new)\n",
    "    \n",
    "    \n",
    "    assert all([count > -1 for count in new.upstream.values()]), new.details()\n",
    "    assert all([count > -1 for count in new.downstream.values()]), new.details()\n",
    "\n",
    "    \n",
    "    ## anchor.specimens.difference_update(prev_node.specimens) REASON?\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed?\n",
    "def update_transition(node):\n",
    "    if node is not nothing_node:\n",
    "        running = node.upstream.keys()\n",
    "        node.upstream = defaultdict(lambda: 0)\n",
    "        for n in running:\n",
    "            if n is not nothing_node:\n",
    "                node.upstream[n] = len(node.specimens.intersection(n.specimens))\n",
    "             \n",
    "        running = node.downstream.keys()\n",
    "        node.downstream = defaultdict(lambda: 0)\n",
    "        for n in running:\n",
    "            if n is not nothing_node:\n",
    "                node.downstream[n] = len(node.specimens.intersection(n.specimens))\n",
    "                \n",
    "        accounted_upstream = sum(node.upstream.values()) - node.upstream[nothing_node]\n",
    "        node.upstream[nothing_node] = len(node.specimens) - accounted_upstream\n",
    "        accounted_downstream = sum(node.downstream.values()) - node.downstream[nothing_node]\n",
    "        node.downstream[nothing_node] = len(node.specimens) - accounted_downstream \n",
    "\n",
    "    assert sum(node.upstream.values()) == len(node.specimens), node.details()\n",
    "    \n",
    "    return node\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_graph = summary2  # deepcopy(\n",
    "example = test_graph[7]\n",
    "original = deepcopy(example)\n",
    "print(example.details())\n",
    "def test_split_one_group(prev_node, anchor, next_node):\n",
    "    x = split_one_group(prev_node, anchor, next_node)\n",
    "    assert x\n",
    "    answer = set(int(x)-1 for x in '14  16  19  20  28  56  59  69  88 133 140 155 159 160 175 193 199 201 224 249 252 258 260 267 268 283 292 295 318 322 325 332 341 344 346 351 354 357 362 364 367 373 374 375 381 386 392 393 394 402 403 417 421 424 426 431 434 435 438 442 445 447 452 455 457 462 463 464 467 471 473 475 476 477 478 480 483 484 494 497 501'.split())\n",
    "    assert x.specimens == answer, 'Specimens set does not agree with HaploBlocker' + str(x.specimens.difference(answer))\n",
    "    return x\n",
    "\n",
    "x = test_split_one_group(first(example.upstream),  example, first(example.downstream) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example.details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "original[7].details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_groups(all_nodes):\n",
    "    \"\"\"This is called crossmerge in the R code\"\"\"\n",
    "    number_of_windows = len(first(simplified_individuals))\n",
    "    length = len(all_nodes)# size of global_nodes changes, necessitating this weird loop\n",
    "    for n in range(length):  \n",
    "        node = all_nodes[n]\n",
    "        #check if all transitition upstream match with one of my downstream nodes\n",
    "        #if set(node.upstream.values()) == set(node.downstream.values()): WHY?\n",
    "        if node.start.snp != 0 and node.end.window != number_of_windows: #chr begin or end\n",
    "            if len(node.specimens) > 0:\n",
    "                # Matchup upstream and downstream with specimen identities\n",
    "                for up in tuple(node.upstream.keys()):\n",
    "                    for down in tuple(node.downstream.keys()):\n",
    "\n",
    "                        set1 = up.specimens\n",
    "                        set2 = down.specimens\n",
    "                        if up == nothing_node:\n",
    "                            set1 = node.specimens\n",
    "                            for index in tuple(node.upstream.keys()):\n",
    "                                if index != nothing_node:\n",
    "                                    set1.difference_update(index.specimens)\n",
    "                        if down == nothing_node:\n",
    "                            set2 = node.specimens\n",
    "                            for index in tuple(node.downstream.keys()):\n",
    "                                if index != nothing_node:\n",
    "                                    set2.difference_update(index.specimens) \n",
    "                        if set1 == set2 and len(set1) > 0:\n",
    "                            new_node = split_one_group(up, node, down) # This changes all_nodes?!\n",
    "                            all_nodes.append(new_node)\n",
    "        \n",
    "    filtered = neglect_nodes(all_nodes, 0)\n",
    "    \n",
    "\n",
    "    length2 = len(filtered)\n",
    "    for n in range(length2):\n",
    "        update_transition(filtered[n])\n",
    "    \n",
    "    return filtered\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_one_group(prev_node, anchor, next_node):\n",
    "    \"\"\" Called when up.specimens == down.specimens\"\"\"\n",
    "    # Comment: That is actually the case we want to split up to obtain longer blocks later\n",
    "    # Extension of full windows will take care of potential loss of information later\n",
    "    my_specimens = anchor.specimens\n",
    "    if prev_node is not nothing_node:  # normal case\n",
    "        my_specimens = my_specimens.intersection(prev_node.specimens)\n",
    "    if next_node is not nothing_node:  # normal case\n",
    "        my_specimens = my_specimens.intersection(next_node.specimens)\n",
    "    if prev_node is nothing_node and next_node is nothing_node:  # exceptional: both are nothing node\n",
    "        my_specimens = anchor.specimens\n",
    "        for n in anchor.downstream.keys():\n",
    "            if n is not nothing_node:  # don't remove empty set\n",
    "                my_specimens -= n.specimens\n",
    "        for n in anchor.upstream.keys():\n",
    "            if n is not nothing_node:  # don't remove empty set\n",
    "                my_specimens -= n.specimens\n",
    "    \n",
    "    my_start, my_end = prev_node.start, next_node.end\n",
    "    my_upstream, my_downstream = prev_node.upstream, next_node.downstream\n",
    "    if nothing_node is prev_node:  # Rare case\n",
    "        my_start = anchor.start\n",
    "        my_upstream = anchor.upstream\n",
    "    if nothing_node is next_node:  # Rare case\n",
    "        my_end = anchor.end\n",
    "        my_downstream = anchor.downstream\n",
    "        \n",
    "    # TODO: what about case where more content is joining downstream?\n",
    "    new = Node(777, my_start, my_end, my_upstream, my_downstream, my_specimens)  \n",
    "    new.specimens = my_specimens\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    ## n.upstream/downstream contains the same key multiple times?!\n",
    "    ## My quick fix was to delete all upstream/downstream and just recalculate everything...\n",
    "    \n",
    "    \n",
    "    # Update upstream/downstream + specimens in prev_node, anchor, next_node\n",
    "    if prev_node != nothing_node:\n",
    "        prev_node.specimens -= new.specimens\n",
    "        prev_node = update_transition(prev_node)\n",
    "    \n",
    "    if next_node != nothing_node:\n",
    "        next_node.specimens -= new.specimens\n",
    "        next_node = update_transition(next_node)\n",
    "    \n",
    "    anchor.specimens -= new.specimens\n",
    "    anchor = update_transition(anchor)\n",
    "    \n",
    "    # Update upstream/downstream for new node\n",
    "    running = new.upstream.keys()\n",
    "    new.upstream = defaultdict(lambda: 0) \n",
    "    for n in running:\n",
    "        if n != nothing_node:\n",
    "            new.upstream[n] = len(new.specimens.intersection(n.specimens))\n",
    "            n.downstream[new] = new.upstream[n]\n",
    "            update_transition(n)\n",
    "        update_transition(new)\n",
    "    \n",
    "    \n",
    "    running = new.downstream.keys()\n",
    "    new.downstream = defaultdict(lambda: 0)\n",
    "    for n in running:\n",
    "        if n != nothing_node:\n",
    "            new.downstream[n] = len(new.specimens.intersection(n.specimens))\n",
    "            n.upstream[new] = new.downstream[n]\n",
    "            update_transition(n)\n",
    "        update_transition(new)\n",
    "    \n",
    "    \n",
    "    assert all([count > -1 for count in new.upstream.values()]), new.details()\n",
    "    assert all([count > -1 for count in new.downstream.values()]), new.details()\n",
    "\n",
    "    \n",
    "    ## anchor.specimens.difference_update(prev_node.specimens) REASON?\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_transition(node):\n",
    "    if node is not nothing_node:\n",
    "        running = node.upstream.keys()\n",
    "        node.upstream = defaultdict(lambda: 0)\n",
    "        for n in running:\n",
    "            if n is not nothing_node:\n",
    "                node.upstream[n] = len(node.specimens.intersection(n.specimens))\n",
    "             \n",
    "        running = node.downstream.keys()\n",
    "        node.downstream = defaultdict(lambda: 0)\n",
    "        for n in running:\n",
    "            if n is not nothing_node:\n",
    "                node.downstream[n] = len(node.specimens.intersection(n.specimens))\n",
    "                \n",
    "        accounted_upstream = sum(node.upstream.values()) - node.upstream[nothing_node]\n",
    "        node.upstream[nothing_node] = len(node.specimens) - accounted_upstream\n",
    "        accounted_downstream = sum(node.downstream.values()) - node.downstream[nothing_node]\n",
    "        node.downstream[nothing_node] = len(node.specimens) - accounted_downstream \n",
    "    return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_split_groups(all_nodes):\n",
    "    summary3 = split_groups(all_nodes)\n",
    "    assert summary3\n",
    "    return summary3\n",
    "summary3 = test_split_groups(summary2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(summary2), len(summary3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary3[1400].details())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Executions Necessary for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Executions Necessary for testing without side effects\n",
    "# alleles, individuals = read_data()\n",
    "\n",
    "test_signatures = get_all_signatures(alleles, individuals)\n",
    "unique_signatures = test_signatures\n",
    "test_individuals = build_individuals(individuals, test_signatures)\n",
    "populate_transitions(test_individuals) # no return val\n",
    "\n",
    "test1 = test_simple_merge(test_signatures)\n",
    "test2 = neglect_nodes(test1)\n",
    "test3 = split_groups(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test1), len(test2), len(test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_individuals[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(summary3) ## Order of nodes does matter here! HaploBlocker output: 1887"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Everything below does not work currently. Some operations on empty sets lead to crashes. \n",
    "#### We should clean up upstream/downstream to not display 0 transition cases. del did not work directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple-merge / Cross-merge runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_signatures = get_all_signatures(alleles, individuals)\n",
    "unique_signatures = test_signatures\n",
    "test_individuals = build_individuals(individuals, test_signatures)\n",
    "populate_transitions(test_individuals) # no return val\n",
    "\n",
    "window_cluster = test_simple_merge(test_signatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_cluster = split_groups(window_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(window_cluster)) ### HB outout after first iteration is 2504 nodes\n",
    "# but we are here merging more so slightly less makes sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_cluster = simple_merge(window_cluster)\n",
    "print(len(window_cluster))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SOMETHINGS GOES TERRIBLY WRONG HERE\n",
    "window_cluster = split_groups(window_cluster)\n",
    "print(len(window_cluster))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    window_cluster = split_groups(window_cluster)\n",
    "    window_cluster = simple_merge(window_cluster)\n",
    "    print(len(window_cluster))\n",
    "    # FINAL value in R-HB here: 2333"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neglect nodes runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    window_cluster = neglect_nodes(window_cluster)\n",
    "    window_cluster = simple_merge(window_cluster)\n",
    "    window_cluster = split_groups(window_cluster)\n",
    "    window_cluster = simple_merge(window_cluster)\n",
    "    print(len(window_cluster))\n",
    "    # FINAL value in R-HB here: 1585"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_cluster = blist([node for window in unique_signatures for node in window.values()])  # think about referencing and deletion\n",
    "window_cluster = simple_merge(window_cluster)\n",
    "#window_cluster = split_groups(window_cluster)\n",
    "#len(window_cluster)\n",
    "#window_cluster[0].details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_cluster[4].details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
