{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pandas import DataFrame, read_csv\n",
    "# read_csv()\n",
    "import os\n",
    "\n",
    "BLOCK_SIZE = 20\n",
    "FILTER_THRESHOLD = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, defaultdict\n",
    "\n",
    "\n",
    "# Point = namedtuple('Point',['window', 'snp', 'bp'])\n",
    "\n",
    "class Point:\n",
    "    def __init__(self, snp, bp=0):\n",
    "        self.snp, self.bp = snp, bp\n",
    "    \n",
    "    @property\n",
    "    def window(self):\n",
    "        return self.snp // BLOCK_SIZE\n",
    "\n",
    "# class Specimen:\n",
    "#     def __init__(self, ident, sequence)\n",
    "#         ident, sequence\n",
    "    \n",
    "class Node:\n",
    "    def __init__(self, ident, start, end, upstream=None, downstream=None):\n",
    "        self.ident = ident\n",
    "        self.start = start #Point()\n",
    "        self.end = end #Point()\n",
    "        # {nothing_node:501, Node: 38,  Node: 201, Node: 3}\n",
    "        self.upstream = defaultdict(lambda: 0) if not upstream else upstream \n",
    "        # {Node: 38,  Node: 201, Node: 3}\n",
    "        self.downstream = defaultdict(lambda: 0) if not downstream else downstream\n",
    "        self.specimens = set()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.specimens)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"N%s(%s, %s)\" % (str(self.ident), str(self.start.snp), str(self.end.snp))\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.ident) + hash(self.start.snp) + hash(self.end.snp)\n",
    "    \n",
    "    def details(self):\n",
    "        return f\"\"\"Node{self.ident}: {self.start.snp} - {self.end.snp}\n",
    "        upstream: { dict((key, value) for key,value in self.upstream.items()) }\n",
    "        downstream: { dict((key, value) for key,value in self.downstream.items()) }\n",
    "        specimens: {self.specimens}\"\"\"\n",
    "        \n",
    "\n",
    "a = Point(0)\n",
    "b = Point(14)\n",
    "str(Node(57, a, b))\n",
    "nothing_node = Node(-1, Point(None), Point(None))\n",
    "global_nodes = {0: nothing_node}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deepcopy(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path = \"../test_data/KE_chromo10.txt\"):\n",
    "    \"\"\"Individuals are rows, not columns\"\"\"\n",
    "    loci = []\n",
    "    with open(file_path) as ke:\n",
    "        for line in ke.readlines():\n",
    "            loci.append(tuple(int(x) for x in line.split()))\n",
    "            \n",
    "    \n",
    "    individuals = np.array(loci).T.tolist()\n",
    "    return loci, individuals\n",
    "alleles, individuals = read_data()\n",
    "assert len(alleles) == 32767\n",
    "assert len(individuals[1]) == 32767\n",
    "assert len(individuals) == 501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first(iterable):\n",
    "    return next(iter(iterable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def signature(individual, start_locus):\n",
    "    return tuple(individual[start_locus : start_locus + BLOCK_SIZE])\n",
    "\n",
    "def get_unique_signatures(individuals, start_locus, block_size = 20):\n",
    "    unique_blocks = {}\n",
    "    for individual in individuals:\n",
    "        sig = signature(individual, start_locus)\n",
    "        if sig not in unique_blocks:\n",
    "            unique_blocks[sig] = Node(len(unique_blocks), Point(start_locus // block_size, start_locus), \n",
    "                                      Point(start_locus // block_size, start_locus + BLOCK_SIZE)) #TODO: -1?\n",
    "    \n",
    "    return unique_blocks\n",
    "unique_blocks = get_unique_signatures(individuals, 0 )\n",
    "    \n",
    "assert len(unique_blocks) == 4\n",
    "unique_blocks\n",
    "# assert unique_blocks == {(0, 2, 0, 0, 2, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 2, 0, 0, 0, 0): 0,\n",
    "#  (0, 0, 2, 2, 0, 2, 0, 2, 2, 2, 0, 2, 2, 2, 0, 0, 2, 2, 2, 2): 1,\n",
    "#  (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0): 2,\n",
    "#  (2, 0, 2, 2, 0, 2, 0, 2, 2, 2, 0, 2, 2, 2, 0, 0, 2, 2, 2, 2): 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_signatures(alleles, individuals):\n",
    "    unique_signatures = []\n",
    "    for locus_start in range(0, len(alleles) - BLOCK_SIZE, BLOCK_SIZE):  # discards remainder \n",
    "        sig = get_unique_signatures(individuals, locus_start, BLOCK_SIZE)\n",
    "        unique_signatures.append(sig)\n",
    "    return unique_signatures\n",
    "unique_signatures = get_all_signatures(alleles, individuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_signatures[21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_individuals(individuals, unique_signatures):\n",
    "    simplified_individuals = []\n",
    "    for i_specimen, specimen in enumerate(individuals):\n",
    "        my_simplification = []\n",
    "        for w, window in enumerate(unique_signatures):  # the length of the genome\n",
    "            sig = signature(specimen, w * BLOCK_SIZE)\n",
    "    #         print(sig, unique_signatures[w][sig])\n",
    "    #         print(i_specimen, window)\n",
    "            my_simplification.append(unique_signatures[w][sig])\n",
    "        simplified_individuals.append(my_simplification)\n",
    "    return simplified_individuals\n",
    "simplified_individuals = build_individuals(individuals, unique_signatures)\n",
    "print(simplified_individuals[500][:100])\n",
    "len(simplified_individuals), len(simplified_individuals[60])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nodes: Populate upstream and downstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build nodes:  first 4 are the 4 starting signatures in window 0.  \n",
    "# For each node list which individuals are present at that node\n",
    "# List transition rates from one node to all other upstream and downstream\n",
    "def populate_transitions(simplified_individuals):\n",
    "    for i, indiv in enumerate(simplified_individuals):\n",
    "        # look what variants are present\n",
    "        for x, node in enumerate(indiv):\n",
    "            node.specimens.add(i)\n",
    "            if x + 1 < len(indiv):\n",
    "                node.downstream[indiv[x+1]] += 1\n",
    "            else:\n",
    "                node.downstream[nothing_node] += 1\n",
    "            if x-1 >= 0:\n",
    "                node.upstream[indiv[x-1]] += 1\n",
    "            else: \n",
    "                node.upstream[nothing_node] += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_signatures = get_all_signatures(alleles, individuals)\n",
    "simplified_individuals = build_individuals(individuals, unique_signatures)\n",
    "populate_transitions(simplified_individuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO: turn these into tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "simplified_individuals[50][0].downstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplified_individuals[49][0].downstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplified_individuals[500][0].downstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplified_individuals[91][0].downstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x.downstream.values() for x in unique_signatures[1000].values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x.upstream.values() for x in unique_signatures[1000].values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add signature directly to node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blist import blist\n",
    "from copy import copy, deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_no_duplicate_nodes(global_nodes):\n",
    "    unique_nodes = set()\n",
    "    for node in global_nodes:\n",
    "        if node in unique_nodes:\n",
    "            print(node)\n",
    "        else:\n",
    "            unique_nodes.add(node)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zoom_stack = [[]]\n",
    "def simple_merge(global_nodes):\n",
    "    new_layer = []  # TODO: copy old nodes to new layer conditionally\n",
    "    n = 0\n",
    "    while n < len(global_nodes):  # size of global_nodes changes, necessitating this weird loop\n",
    "        node = global_nodes[n]\n",
    "    #     print(node, type(node))\n",
    "        if len(node.downstream) == 1: \n",
    "            next_node = first(node.downstream.keys())\n",
    "            if len(node.specimens) == len(next_node.specimens):\n",
    "                #Torsten deletes nodeA and modifies next_node\n",
    "                next_node.upstream = node.upstream\n",
    "                next_node.start = node.start\n",
    "                #prepare to delete node by removing references\n",
    "                for parent in node.upstream.keys():\n",
    "                    if parent != nothing_node:\n",
    "                        count = parent.downstream[node]\n",
    "                        del parent.downstream[node]  # updating pointer \n",
    "                        parent.downstream[next_node] = count \n",
    "                global_nodes.remove(node)  #delete node\n",
    "                # zoom_stack[0].append(merged)\n",
    "                n -= 1\n",
    "        n += 1\n",
    "    return global_nodes        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_simple_merge():\n",
    "    global_nodes = blist([node for window in unique_signatures for node in window.values()])  # think about referencing and deletion\n",
    "    assert len(global_nodes) == 7180\n",
    "    summary1 = simple_merge(global_nodes)\n",
    "    assert len(summary1) == 3690\n",
    "    return summary1\n",
    "summary1 = test_simple_merge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neglect Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_node(node, cutoff):\n",
    "    \"\"\"Changes references to this node to add to references to nothing_node\"\"\"\n",
    "    if cutoff < 1:\n",
    "        return  # if cutoff is 0, then don't touch upstream and downstream\n",
    "    for parent, count in node.upstream.items():\n",
    "        parent.downstream[nothing_node] += parent.downstream[node]\n",
    "        del parent.downstream[node]\n",
    "    for descendant, count in node.downstream.items():\n",
    "        descendant.upstream[nothing_node] += descendant.upstream[node]\n",
    "        del descendant.upstream[node]\n",
    "        \n",
    "\n",
    "def neglect_nodes(all_nodes, deletion_cutoff=FILTER_THRESHOLD):\n",
    "    nodes_to_delete = set()\n",
    "#     filtered_nodes = copy(all_nodes)\n",
    "#     filtered_nodes.remove(1)\n",
    "#     assert len(all_nodes) != len(filtered_nodes)\n",
    "    for node in all_nodes:\n",
    "        if len(node.specimens) <= deletion_cutoff:\n",
    "            delete_node(node, deletion_cutoff)  # TODO: check if this will orphan \n",
    "            nodes_to_delete.add(node)\n",
    "    filtered_nodes = blist([x for x in all_nodes if x not in nodes_to_delete])\n",
    "    # TODO: remove orphaned haplotypes in a node that transition to and from zero within a 10 window length\n",
    "    return filtered_nodes \n",
    "\n",
    "\n",
    "def test_neglect_nodes(all_nodes):\n",
    "    summary2 = neglect_nodes(all_nodes)\n",
    "    assert len(summary2) == 2854\n",
    "    unchanged = neglect_nodes(summary2, 0)\n",
    "    assert len([n for n in unchanged if len(n.specimens) == 0]) == 0\n",
    "    return summary2\n",
    "summary2 = test_neglect_nodes(summary1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary2[8].details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_one_group(prev_node, anchor, next_node):\n",
    "    \"\"\" Called when up.specimens == down.specimens\"\"\"\n",
    "    new = Node(777, prev_node.start, next_node.end, prev_node.upstream, next_node.downstream)  # TODO: what about case where more content is joining downstream?\n",
    "    # Comment: That is actually the case we want to split up to obtain longer blocks later\n",
    "    # Extension of full windows will take care of potential loss of information later\n",
    "    \n",
    "    if nothing_node != prev_node:\n",
    "        new.specimens = anchor.specimens.intersection(prev_node.specimens)\n",
    "    elif nothing_node != next_node:\n",
    "        new.specimens = anchor.specimens.intersection(next_node.specimens)\n",
    "    else:\n",
    "        new.specimens = anchor.specimens\n",
    "        for n in next_node.downstream.keys():\n",
    "            if n != nothing_node:\n",
    "                new.specimens = new.specimens.remove(n.specimens)\n",
    "        for n in prev_node.upstream.keys():\n",
    "            if n != nothing_node:\n",
    "                new.specimens = new.specimens.remove(n.specimens)\n",
    "    \n",
    "    if nothing_node is prev_node:  # Rare case\n",
    "        new.start = anchor.start\n",
    "        new.upstream = anchor.upstream\n",
    "    if nothing_node is next_node:\n",
    "        new.end = anchor.end\n",
    "        new.downstream = anchor.downstream\n",
    "        \n",
    "\n",
    "    \n",
    "    print(new.details())\n",
    "    print(new.upstream.keys())\n",
    "    print(new.upstream.values())\n",
    "    print(sum(new.upstream.values()))\n",
    "    \n",
    "    # Update upstream/downstream\n",
    "    running = new.upstream.keys()\n",
    "\n",
    "    ## n.upstream/downstream contains the same key multiple times?!\n",
    "    ## My quick fix was to delete all upstream/downstream and just recalculate everything...\n",
    "    new.upstream = defaultdict(lambda: 0) \n",
    "    for n in running:\n",
    "        if n != nothing_node:\n",
    "            new.upstream[n] = len(new.specimens.intersection(n.specimens))\n",
    "            n.downstream[new] = new.upstream[n]\n",
    "            n.downstream[prev_node] = n.downstream[prev_node] - n.downstream[new]\n",
    "            if n.downstream[prev_node] == 0:\n",
    "                del n.downstream[prev_node]\n",
    "    \n",
    "    running = new.downstream.keys()\n",
    "    new.downstream = defaultdict(lambda: 0)\n",
    "    for n in running:\n",
    "        if n != nothing_node:\n",
    "            new.downstream[n] = len(new.specimens.intersection(n.specimens))\n",
    "            n.upstream[new] = new.downstream[n]\n",
    "            n.upstream[next_node] = n.upstream[next_node] - n.upstream[new]\n",
    "            if n.upstream[next_node] == 0:\n",
    "                del n.upstream[next_node]\n",
    "    \n",
    "    print(new.details())\n",
    "    print(new.upstream.keys())\n",
    "    print(new.upstream.values())\n",
    "    print(sum(new.upstream.values()))\n",
    "    \n",
    "    accounted_upstream = sum(new.upstream.values()) - new.upstream[nothing_node]\n",
    "    #print(f'upstream {sum(new.upstream.values())} downstream {sum(new.downstream.values())}')\n",
    "    new.upstream[nothing_node] = len(new.specimens) - accounted_upstream\n",
    "    accounted_downstream = sum(new.downstream.values()) - new.downstream[nothing_node]\n",
    "    new.downstream[nothing_node] = len(new.specimens) - accounted_downstream \n",
    "    \n",
    "    assert all([count > -1 for count in new.upstream.values()]), new.details()\n",
    "    assert all([count > -1 for count in new.downstream.values()]), new.details()\n",
    "    # Update Specimens in prev_node, anchor, next_node\n",
    "    if prev_node != nothing_node:\n",
    "        prev_node.specimens -= new.specimens\n",
    "    \n",
    "    if next_node != nothing_node:\n",
    "        next_node.specimens -= new.specimens\n",
    "    \n",
    "    anchor.specimens -= new.specimens\n",
    "        \n",
    "    ## anchor.specimens.difference_update(prev_node.specimens) REASON?\n",
    "    return new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_graph = summary2  # deepcopy(\n",
    "example = test_graph[7]\n",
    "original = deepcopy(example)\n",
    "print(example.details())\n",
    "def test_split_one_group(prev_node, anchor, next_node):\n",
    "    x = split_one_group(prev_node, anchor, next_node)\n",
    "    assert x\n",
    "    answer = set(int(x)-1 for x in '14  16  19  20  28  56  59  69  88 133 140 155 159 160 175 193 199 201 224 249 252 258 260 267 268 283 292 295 318 322 325 332 341 344 346 351 354 357 362 364 367 373 374 375 381 386 392 393 394 402 403 417 421 424 426 431 434 435 438 442 445 447 452 455 457 462 463 464 467 471 473 475 476 477 478 480 483 484 494 497 501'.split())\n",
    "    assert x.specimens == answer, 'Specimens set does not agree with HaploBlocker' + str(x.specimens.difference(answer))\n",
    "    return x\n",
    "\n",
    "x = test_split_one_group(first(example.upstream),  example, first(example.downstream) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example.details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "original[7].details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_groups(all_nodes):\n",
    "    \"\"\"This is called crossmerge in the R code\"\"\"\n",
    "    number_of_windows = len(first(simplified_individuals))\n",
    "    length = len(all_nodes)# size of global_nodes changes, necessitating this weird loop\n",
    "    for n in range(length):  \n",
    "        node = all_nodes[n]\n",
    "        #check if all transitition upstream match with one of my downstream nodes\n",
    "        #if set(node.upstream.values()) == set(node.downstream.values()): WHY?\n",
    "        if node.start.snp != 0 and node.end.window != number_of_windows: #chr begin or end\n",
    "            if len(node.specimens) > 0:\n",
    "                # Matchup upstream and downstream with specimen identities\n",
    "                for up in tuple(node.upstream.keys()):\n",
    "                    for down in tuple(node.downstream.keys()):\n",
    "\n",
    "                        set1 = up.specimens\n",
    "                        set2 = down.specimens\n",
    "                        if up == nothing_node:\n",
    "                            set1 = node.specimens\n",
    "                            for index in tuple(node.upstream.keys()):\n",
    "                                set1.intersection(index.specimens) # =- does not work for empty sets\n",
    "                        if down == nothing_node:\n",
    "                            set2 = node.specimens\n",
    "                            for index in tuple(node.downstream.keys()):\n",
    "                                set2.intersection(index.specimens) # =- does not work for empty sets\n",
    "\n",
    "                        if set1 == set2 and len(set1) > 0:\n",
    "                            new_node = split_one_group(up, node, down)\n",
    "                            all_nodes.append(new_node)\n",
    "        \n",
    "    filtered = neglect_nodes(all_nodes, 0)\n",
    "    return filtered\n",
    "    \n",
    "\n",
    "def test_split_groups(all_nodes):\n",
    "    summary3 = split_groups(all_nodes)\n",
    "    assert summary3\n",
    "    return summary3\n",
    "summary3 = test_split_groups(summary2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(summary3) ## Order of nodes does matter here! HaploBlocker output: 1887"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Everything below does not work currently. Some operations on empty sets lead to crashes. \n",
    "#### We should clean up upstream/downstream to not display 0 transition cases. del did not work directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple-merge / Cross-merge runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_cluster = blist([node for window in unique_signatures for node in window.values()])  # think about referencing and deletion\n",
    "len(window_cluster)\n",
    "\n",
    "for i in range(10):\n",
    "    window_cluster = simple_merge(window_cluster)\n",
    "    window_cluster = split_groups(window_cluster)\n",
    "    print(len(window_cluster))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(window_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neglect nodes runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    window_cluster = neglect_nodes(window_cluster)\n",
    "    window_cluster = simple_merge(window_cluster)\n",
    "    window_cluster = split_groups(window_cluster)\n",
    "    window_cluster = simple_merge(window_cluster)\n",
    "\n",
    "    print(len(window_cluster))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(window_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
